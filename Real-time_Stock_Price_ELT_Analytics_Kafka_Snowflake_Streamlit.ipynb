{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "rt22yrnzahc3mvpdkpen",
   "authorId": "7638589351455",
   "authorName": "lac",
   "authorEmail": "lac@domain.com",
   "sessionId": "e44cac15-6412-4d9d-8f4f-35efb3b36321",
   "lastEditTime": 1747089346298
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "name": "Overview",
    "collapsed": false
   },
   "source": "# ðŸ“ˆ Real-Time Stock Price ELT & Analytics Pipeline with Kafka, Snowflake & Streamlit\n\nThis project illustrates how to stream real-time stock price data from Yahoo Finance into Snowflake using Kafka and visualize it using a Snowflake-hosted Streamlit app.\n\n---\n\n## ðŸš€ Overview\n\n- **Setup**: Setup Kafka and Snowflake Kafka Connector for real-time data streaming.\n- **Ingest**: Publish live stock data from Yahoo Finance to a Kafka topic.\n- **Load**: Stream the data into a Snowflake table using the Kafka Connector, and Snowflake Streams and Tasks.\n- **Transform**: Real-time or near-real-time analytics on stock prices in Snowflake.\n- **Visualize**: Deliver live metrics in a Streamlit dashboard.\n\n---\n"
  },
  {
   "cell_type": "markdown",
   "id": "b54164ff-2a97-412a-bfc1-248b16839027",
   "metadata": {
    "name": "Setup",
    "collapsed": false
   },
   "source": "## Setup Kafka and Snowflake Kafka Connector for real-time data streaming\n"
  },
  {
   "cell_type": "markdown",
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "name": "Setup_Snowflake_4_Kafka",
    "collapsed": false
   },
   "source": "1. Create Snowflake database, schema, table objects, new kafka role and kafka connect user, setup grant and privileges in a Snowflake worksheet.\n\n```sql\nUSE ROLE SYSADMIN;\n\nCREATE OR REPLACE DATABASE KAFKA_STREAMING;\n\nCREATE OR REPLACE SCHEMA YAHOO_FINANCE;\n\n-- Create target table\nCREATE OR REPLACE TABLE KAFKA_STREAMING.YAHOO_FINANCE.STOCK_PRICES (\n  symbol STRING,\n  price FLOAT,\n  currency STRING,\n  time STRING\n);\n\n-- Create and grant a custom kafka role\n\nUSE ROLE ACCOUNTADMIN;\n\nCREATE ROLE kafka_role;\n\n-- Grant required permissions\nGRANT ROLE KAFKA_ROLE TO ROLE SYSADMIN;\n\n\nGRANT USAGE ON DATABASE KAFKA_STREAMING TO ROLE kafka_role;\nGRANT USAGE ON SCHEMA KAFKA_STREAMING.YAHOO_FINANCE TO ROLE kafka_role;\nGRANT INSERT ON TABLE KAFKA_STREAMING.YAHOO_FINANCE.STOCK_PRICES TO ROLE kafka_role;\n\nGRANT OWNERSHIP ON DATABASE KAFKA_STREAMING TO ROLE kafka_role REVOKE CURRENT GRANTS;\nGRANT OWNERSHIP ON SCHEMA KAFKA_STREAMING.YAHOO_FINANCE TO ROLE kafka_role REVOKE CURRENT GRANTS;\nGRANT OWNERSHIP ON TABLE KAFKA_STREAMING.YAHOO_FINANCE.STOCK_PRICES TO ROLE kafka_role REVOKE CURRENT GRANTS;\n\n-- Create kafka connector user\nCREATE USER kafka_connector_user\n  PASSWORD = '****'\n  DEFAULT_ROLE = kafka_role\n  MUST_CHANGE_PASSWORD = FALSE;\n\n-- Assign role to user\nGRANT ROLE kafka_role TO USER kafka_connector_user;\n\nSHOW USERS IN ACCOUNT;\n```"
  },
  {
   "cell_type": "markdown",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "name": "Setup_Kafka",
    "collapsed": false
   },
   "source": "2. Download kafka to local machine from https://kafka.apache.org/downloads.\n\n3. Start `zookeeper` in new terminal.\n\n```bash\ncd kafka/bin\n./zookeeper-server-start.sh ../config/zookeeper.properties\n```\n\n4. Start `kafka server` in new terminal.\n\n```bash\ncd kafka/bin\n./kafka-server-start.sh ../config/server.properties\n```\n\n5. Create `kafka topic` in new terminal.\n```bash\ncd kafka/bin\n./kafka-topics.sh --create --topic yahoo-finance-topic --zookeeper localhost:2181 --partitions 2 --replication-factor 1\n```\n\n\n\n\n\n"
  },
  {
   "cell_type": "markdown",
   "id": "e8c2a568-6323-482b-9991-e8ea0215cd6f",
   "metadata": {
    "name": "Setup_Kafka_Connector",
    "collapsed": false
   },
   "source": "6. Setup Private Key `Authentication` for Snowflake Kafka Connector\n\n- Generate `private key`. In new terminal:\n\n```bash\nopenssl genrsa 2048 | openssl pkcs8 -topk8 -v2 des3 -inform PEM -out snowflake-kafka-connector-private-rsa-key.p8 â€“nocrypt\n```\n\nRemove -nocrypt option to obtain an encrypted private key. Provide the encryption password.\n\n- Generate `public key`.\n\n```bash\nopenssl rsa -in snowflake-kafka-connector-private-rsa-key.p8 -pubout -out snowflake-kafka-connector-public-rsa-key.pub\n```\n\nSave the keys for later use.\n\n7. Add the public key to the `kafka_connect_user`. In the SQL worksheet from step 1\n\n```sql\nALTER USER kafka_connector_user SET RSA_PUBLIC_KEY='MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIB.....';\n```\n\n8. Setup Snowflake Kafka Connector\n\n- Download `Snowflake Kafka Connector` jar file from [Maven Repository](https://mvnrepository.com/artifact/com.snowflake/snowflake-kafka-connector/3.1.0?_fsi=WP9Bbe6o&_fsi=WP9Bbe6o) and copy the jar file into kafka/libs/ folder.\n\n- Start `Kafka Connector` on local machine. In new terminal\n\n```bash\ncd kafka/bin\n./connect-distributed.sh ../config/connect-distributed.properties\n```\n\n- Create Kafka Connect `Sink Connector Configuration` File (`snowflake-kafka-connector-config.json`)\n\n```json\n{\n  \"name\": \"snowflake-kafka-yahoo-finance-connector\",\n  \"config\": {\n    \"connector.class\": \"com.snowflake.kafka.connector.SnowflakeSinkConnector\",\n    \"tasks.max\": \"1\",\n    \"topics\": \"yahoo-finance-topic\",\n\n    \"snowflake.url.name\": \"ACCOUNT-IDENTIFIER.snowflakecomputing.com:443\", -- replace ACCOUNT-IDENTIFIER\n    \"snowflake.user.name\": \"kafka_connector_user\",\n    \"snowflake.private.key\": \"MIIEuwIBADANBgkqhkiG9w0BAQEFAASCBKUwggShA.....\",\n    \"snowflake.database.name\": \"KAFKA_STREAMING\",\n    \"snowflake.schema.name\": \"YAHOO_FINANCE\",\n    \"snowflake.table.name\": \"stock_prices\",\n    \"snowflake.role.name\": \"kafka_role\",\n\n    \"key.converter\": \"org.apache.kafka.connect.storage.StringConverter\",\n    \"value.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\n    \"value.converter.schemas.enable\": \"false\",\n\n    \"buffer.count.records\": \"1000\",\n    \"buffer.flush.time\": \"10\",\n    \"buffer.size.bytes\": \"5000000\",\n\n    \"behavior.on.null.values\": \"IGNORE\"\n  }\n}\n```\n\n9. Deploy configuration and create the Kafka Connector via REST API. In new terminal\n\n```bash\ncurl -X POST -H \"Content-Type: application/json\" --data @snowflake-kafka-connector-config.json http://localhost:8083/connectors\n```"
  },
  {
   "cell_type": "markdown",
   "id": "4dd0fc6d-23b7-48bf-8270-7015a865058a",
   "metadata": {
    "name": "Ingest",
    "collapsed": false
   },
   "source": "## Publish live stock data from Yahoo Finance to yahoo-finance-topic Kafka topic\n"
  },
  {
   "cell_type": "markdown",
   "id": "8057eff5-4b64-4868-a43b-ca35b91923f9",
   "metadata": {
    "name": "Create_Kafka_Producer",
    "collapsed": false
   },
   "source": "Create `kafka producer` to fetch live stock prices and publish them to Kafka.\n\n1. Install required Python packages.\n\n```bash\npip install yfinance confluent_kafka\n```\n\n2. Create Python script `kafka-producer.py` to fetch data and publish \n\n```python\nimport yfinance as yf \nfrom confluent_kafka import Producer \nimport json\nimport time\n\n# Kafka config\nproducer = Producer({'bootstrap.servers': 'localhost:9092'})\ntopic = 'yahoo-finance-topic'\n\n# List of stock symbols to track\nsymbols = ['SNOW', 'AMZN', 'GOOGL', 'MSFT']\n\ndef acked(err, msg):\n    if err is not None:\n        print(f\"Failed to deliver message: {err}\")\n    else:\n        print(f\"Published to {msg.topic()} [{msg.partition()}] @ offset {msg.offset()}\")\n\nwhile True:\n    for symbol in symbols:\n        stock = yf.Ticker(symbol)\n        data = stock.info  # full info\n        \n        message = {\n            'symbol': symbol,\n            'price': data.get('regularMarketPrice'),\n            'currency': data.get('currency'),\n            'time': time.strftime('%Y-%m-%d %H:%M:%S'),\n        }\n\n        producer.produce(topic, value=json.dumps(message), key=symbol, callback=acked)\n    \n    producer.flush()\n    time.sleep(30)  # Fetch every 30 seconds\n```\n\n3. Run kafka-producer.py script.\n\n```bash\npython kafka-producer.py\n```"
  },
  {
   "cell_type": "markdown",
   "id": "7baa60da-0a23-442c-ba7b-527c5450770c",
   "metadata": {
    "name": "Load",
    "collapsed": false
   },
   "source": "## Stream the data into a Snowflake table using the Kafka Connector\n"
  },
  {
   "cell_type": "markdown",
   "id": "70dbb92a-e801-4a77-a8a7-7cc5bf006314",
   "metadata": {
    "name": "Kafka_streaming_raw",
    "collapsed": false
   },
   "source": "Once the Kafka Producer is publishing Yahoo Finance data to the Kafka topic and the Snowflake Kafka Connector is running, Snowflake will automatically ingest data.\n\nBy default, the Snowflake Kafka Connector does not write directly to the specified target table `stock_prices` unless the Kafka message schema exactly matches the table schema.\n\nIf the schema-maching mode is not enforced, Kafka auto-generates, by default, in the KAFKA_STREAMING.YAHOO_FINANCE schema:\n\n- A `staging table`: \n```sql\nCREATE OR REPLACE KAFKA_STREAMING.YAHOO_FINANCE.YAHOO_FINANCE_TOPIC_1140052305 (\n    RECORD_METADATA VARIANT,\n    RECORD_CONTENT VARIANT)\n```\n\n- An `internal stage` with client-side encryption and directory disabled : \n```sql\nCREATE OR REPLACE STAGE SNOWFLAKE_KAFKA_CONNECTOR_SNOWFLAKE_KAFKA_YAHOO_FINANCE_CONNECTOR_430186219_STAGE_YAHOO_FINANCE_TOPIC_1140052305;\n```\n\n- A `Pipe`: \n\n```sql\nCREATE OR REPLACE PIPE KAFKA_STREAMING.YAHOO_FINANCE.SNOWFLAKE_KAFKA_CONNECTOR_SNOWFLAKE_KAFKA_YAHOO_FINANCE_CONNECTOR_430186219_PIPE_YAHOO_FINANCE_TOPIC_1140052305_0 auto_ingest=false as copy into yahoo_finance_topic_1140052305(RECORD_METADATA, RECORD_CONTENT) from (select $1:meta, $1:content from @SNOWFLAKE_KAFKA_CONNECTOR_snowflake_kafka_yahoo_finance_connector_430186219_STAGE_yahoo_finance_topic_1140052305 t) file_format = (type = 'json');\n```"
  },
  {
   "cell_type": "markdown",
   "id": "b4b2dbce-79c8-41da-bf03-005ca73f5ab8",
   "metadata": {
    "name": "Kafka_streaming_curated",
    "collapsed": false
   },
   "source": "Automate the movement of data from the Kafka Connector auto-created staging table into the stock_prices target table using Snowflake Streams and Tasks. \n\nThis approach:\n\n- Avoids directly ingesting into the stock_prices curated table\n\n- Keeps staging and production concerns cleanly separated\n\n- Runs automatically at intervals (e.g., every 1 minute)\n\n1. Identify the Auto-Generated Table\n\n```sql\nSHOW TABLES LIKE '%_TOPIC_%';\n\nSELECT * FROM YAHOO_FINANCE_TOPIC_1140052305;\n\n-- TABLE name from SHOW TABLES\nSET kafka_staging_table = 'YAHOO_FINANCE_TOPIC_1140052305';\n```\n2. Create a Stream on the Auto Table\n\nThe stream tracks new rows inserted into the staging table by the Kafka Connector.\n\n```sql\nCREATE OR REPLACE STREAM kafka_finance_stream\nON TABLE IDENTIFIER($kafka_staging_table);\n```\n\n3. Create a Task to Copy Data Every Minute\n\n```sql\nCREATE OR REPLACE TASK move_kafka_data_to_snowflake_stock_prices\n  WAREHOUSE = COMPUTE_WH  \n  SCHEDULE = '1 MINUTE'\nAS\nINSERT INTO stock_prices (symbol, price, currency, time)\nSELECT\n  RECORD_CONTENT:\"symbol\"::STRING,\n  RECORD_CONTENT:\"price\"::FLOAT,\n  RECORD_CONTENT:\"currency\"::STRING,\n  RECORD_CONTENT:\"time\"::STRING\nFROM kafka_finance_stream;\n```\n\n4. Start the task\n\n```sql\nALTER TASK move_kafka_data_to_snowflake_stock_prices RESUME;\n```\nNow, Snowflake will:\n\n- Continuously ingest data into the auto-generated Kafka staging table\n\n- Use the stream to detect changes\n\n- Copy those rows into the curated stock_prices table every minute\n\n5. Verification -- after 1 minute\n\n```sql\nSELECT * FROM stock_prices ORDER BY time DESC;\n```\n\n\n\n"
  },
  {
   "cell_type": "markdown",
   "id": "5c25729e-be96-4d48-8958-cb597f0c31ed",
   "metadata": {
    "name": "Transform",
    "collapsed": false
   },
   "source": "## Real-time or near-real-time analytics on stock prices, ranging from simple aggregations to more advanced time-series and anomaly detection\n"
  },
  {
   "cell_type": "markdown",
   "id": "1ed372b2-d080-4801-b4e8-a75ea970cc3b",
   "metadata": {
    "name": "Analytics",
    "collapsed": false
   },
   "source": "```sql\nUSE ROLE ACCOUNTADMIN;\nUSE DATABASE KAFKA_STREAMING;\nUSE SCHEMA YAHOO_FINANCE;\n```\n\n1. Latest price per symbol\n\n```sql\nCREATE OR REPLACE VIEW vw_latest_stock_prices AS\nSELECT symbol, price, time\nFROM (\n  SELECT *,\n         ROW_NUMBER() OVER (PARTITION BY symbol ORDER BY time DESC) AS rn\n  FROM stock_prices\n)\nWHERE rn = 1;\n```\n\n2. Moving average (5-minute window)\n\n```sql\nCREATE OR REPLACE VIEW vw_5min_moving_avg AS\nSELECT \n  symbol,\n  DATE_TRUNC('minute', time::TIMESTAMP_NTZ) AS minute_bucket,\n  AVG(price) AS avg_price_5min\nFROM stock_prices\nWHERE time::TIMESTAMP_NTZ >= DATEADD(HOUR, -1, CURRENT_TIMESTAMP)\nGROUP BY symbol, minute_bucket;\n```\n\n3. Anomaly detection - Price spike/dip detection (5% deviation from 5-row moving avg)\n\n```sql\nCREATE OR REPLACE VIEW vw_price_anomalies AS\nWITH recent AS (\n  SELECT symbol, price, time,\n         AVG(price) OVER (PARTITION BY symbol ORDER BY time ROWS BETWEEN 4 PRECEDING AND CURRENT ROW) AS moving_avg\n  FROM stock_prices\n)\nSELECT *\nFROM recent\nWHERE ABS(price - moving_avg) / NULLIF(moving_avg, 0) > 0.05;\n```\n\n4. Hourly trend (avg price) - time-series windowed comparison to compare average prices hour-over-hour\n\n```sql\nCREATE OR REPLACE VIEW vw_hourly_avg_prices AS\nSELECT \n  symbol,\n  DATE_TRUNC('hour', time::TIMESTAMP_NTZ) AS hour_bucket,\n  AVG(price) AS avg_price_hour\nFROM stock_prices\nWHERE time::TIMESTAMP_NTZ >= DATEADD(DAY, -1, CURRENT_TIMESTAMP)\nGROUP BY symbol, hour_bucket;\n```\n\n5. Stock leaderboard by latest price - ranks latest prices to show top movers or high-value stocks\n\n```sql\nCREATE OR REPLACE VIEW vw_price_leaderboard AS\nSELECT symbol, price, RANK() OVER (ORDER BY price DESC) AS price_rank\nFROM (\n  SELECT symbol, price\n  FROM stock_prices\n  QUALIFY ROW_NUMBER() OVER (PARTITION BY symbol ORDER BY time DESC) = 1\n);\n```\n\nThe Snowflake QUALIFY clause is a tool used to filter results of window functions in SQL queries. Window functions perform calculations across a set of table rows related to the current row, and QUALIFY acts as an additional filter after these calculations. QUALIFY can improve query performance by reducing the need for subqueries and intermediate result sets. This can lead to faster query execution times and more efficient use of system resources, particularly in large datasets where performance is a critical concern."
  },
  {
   "cell_type": "markdown",
   "id": "a0b902d2-1910-4a79-a114-427f445e1722",
   "metadata": {
    "name": "Visualize",
    "collapsed": false
   },
   "source": "## Deliver live metrics in a Streamlit dashboard\n"
  },
  {
   "cell_type": "markdown",
   "id": "cd4c1b6f-5c3e-448f-a3bb-4b4b72fb122f",
   "metadata": {
    "name": "Streamlit_app",
    "collapsed": false
   },
   "source": "```python\nimport streamlit as st\nimport pandas as pd\nimport altair as alt\nfrom snowflake.snowpark import Session\n\n# Initialize Snowpark session\nsession = Session.get_active_session()\n\n# Load data\nlatest = session.table(\"vw_latest_stock_prices\").to_pandas()\nmoving_avg = session.table(\"vw_5min_moving_avg\").to_pandas()\nanomalies = session.table(\"vw_price_anomalies\").to_pandas()\nhourly = session.table(\"vw_hourly_avg_prices\").to_pandas()\nleaderboard = session.table(\"vw_price_leaderboard\").to_pandas()\n\n# Streamlit UI components\nst.title(\"ðŸ“ˆ Yahoo Finance Dashboard\")\nst.dataframe(latest)\nst.dataframe(leaderboard)\n\n# Moving Average Chart\nsymbol = st.selectbox(\"Select Symbol\", moving_avg['SYMBOL'].unique())\nfiltered_data = moving_avg[moving_avg['SYMBOL'] == symbol]\n\nchart = alt.Chart(filtered_data).mark_line(point=True).encode(\n    x=alt.X('MINUTE_BUCKET:T', title='Time'),\n    y=alt.Y('AVG_PRICE_5MIN:Q', title='5-Min Avg Price'),\n    tooltip=['MINUTE_BUCKET:T', 'AVG_PRICE_5MIN:Q']\n).properties(\n    title=f\"5-Minute Average for {symbol}\",\n    height=300\n)\n\nst.altair_chart(chart)\n\n# Anomalies Table\nst.dataframe(anomalies[anomalies['SYMBOL'] == symbol])\n```"
  }
 ]
}