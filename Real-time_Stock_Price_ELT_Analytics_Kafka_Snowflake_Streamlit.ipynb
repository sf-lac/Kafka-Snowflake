{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "gzjtc6vgdvax44mwkxji",
   "authorId": "4910449919594",
   "authorName": "SNOWWHITE2025",
   "authorEmail": "lac@mechatronix.ca",
   "sessionId": "b1d357ce-5416-4a63-93d7-d0ad427cb4e3",
   "lastEditTime": 1758147881200
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "name": "Overview",
    "collapsed": false
   },
   "source": "# ðŸ“ˆ Real-Time Stock Price ELT & Analytics Pipeline with Kafka, Snowflake & Streamlit\n\nThis project illustrates how to stream real-time stock price data from Yahoo Finance into Snowflake using Kafka and visualize it using a Snowflake-hosted Streamlit app.\n\n---\n\n## ðŸš€ Overview\n\n- **Setup**: Setup Kafka and Snowflake Kafka Connector for real-time data streaming.\n- **Ingest**: Publish live stock data from Yahoo Finance to a Kafka topic.\n- **Load**: Stream the data into a Snowflake table using the Kafka Connector, and Snowflake Streams and Tasks.\n- **Transform**: Real-time or near-real-time analytics on stock prices in Snowflake.\n- **Visualize**: Deliver live metrics in a Streamlit dashboard.\n\n---\n"
  },
  {
   "cell_type": "markdown",
   "id": "b54164ff-2a97-412a-bfc1-248b16839027",
   "metadata": {
    "name": "Setup",
    "collapsed": false
   },
   "source": "## Setup Kafka and Snowflake Kafka Connector for real-time data streaming\n"
  },
  {
   "cell_type": "markdown",
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "name": "Setup_Snowflake_4_Kafka",
    "collapsed": false
   },
   "source": "__1. Create Snowflake database, schema, table objects, new kafka role and kafka connect user, setup grant and privileges in a Snowflake worksheet.__\n\n```sql\nUSE ROLE SYSADMIN;\n\nCREATE OR REPLACE DATABASE KAFKA_STREAMING;\n\nCREATE OR REPLACE SCHEMA YAHOO_FINANCE;\n\n-- Create target table\nCREATE OR REPLACE TABLE KAFKA_STREAMING.YAHOO_FINANCE.STOCK_PRICES (\n  symbol STRING,\n  price FLOAT,\n  currency STRING,\n  time STRING\n);\n\n-- Create and grant a custom kafka role\n\nUSE ROLE ACCOUNTADMIN;\n\nCREATE ROLE kafka_role;\n\n-- Grant required permissions\nGRANT ROLE KAFKA_ROLE TO ROLE SYSADMIN;\n\n\nGRANT USAGE ON DATABASE KAFKA_STREAMING TO ROLE kafka_role;\nGRANT USAGE ON SCHEMA KAFKA_STREAMING.YAHOO_FINANCE TO ROLE kafka_role;\nGRANT INSERT ON TABLE KAFKA_STREAMING.YAHOO_FINANCE.STOCK_PRICES TO ROLE kafka_role;\n\nGRANT OWNERSHIP ON DATABASE KAFKA_STREAMING TO ROLE kafka_role REVOKE CURRENT GRANTS;\nGRANT OWNERSHIP ON SCHEMA KAFKA_STREAMING.YAHOO_FINANCE TO ROLE kafka_role REVOKE CURRENT GRANTS;\nGRANT OWNERSHIP ON TABLE KAFKA_STREAMING.YAHOO_FINANCE.STOCK_PRICES TO ROLE kafka_role REVOKE CURRENT GRANTS;\n\n-- Create kafka connector user\nCREATE USER kafka_connector_user\n  PASSWORD = '****'\n  DEFAULT_ROLE = kafka_role\n  MUST_CHANGE_PASSWORD = FALSE;\n\n-- Assign role to user\nGRANT ROLE kafka_role TO USER kafka_connector_user;\n\nSHOW USERS IN ACCOUNT;\n```"
  },
  {
   "cell_type": "markdown",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "name": "Setup_Kafka",
    "collapsed": false
   },
   "source": "__2. Download kafka to local machine from https://kafka.apache.org/downloads.__\n\n__3. Start `zookeeper` in new terminal.__\n\n```bash\ncd kafka/bin\n./zookeeper-server-start.sh ../config/zookeeper.properties\n```\n\n__4. Start `kafka server` in new terminal.__\n\n```bash\ncd kafka/bin\n./kafka-server-start.sh ../config/server.properties\n```\n\n__5. Create `kafka topic` in new terminal.__\n```bash\ncd kafka/bin\n./kafka-topics.sh --create --topic yahoo-finance-topic --zookeeper localhost:2181 --partitions 2 --replication-factor 1\n```\n\n\n\n\n\n"
  },
  {
   "cell_type": "markdown",
   "id": "e8c2a568-6323-482b-9991-e8ea0215cd6f",
   "metadata": {
    "name": "Setup_Kafka_Connector",
    "collapsed": false
   },
   "source": "__6. Setup `Private Key Authentication` for Snowflake Kafka Connector__\n\n- Generate `private key` in new terminal.\n\n```bash\nopenssl genrsa 2048 | openssl pkcs8 -topk8 -v2 des3 -inform PEM -out snowflake-kafka-connector-private-rsa-key.p8 â€“nocrypt\n```\n\nRemove -nocrypt option to obtain an encrypted private key. Provide the encryption password.\n\n- Generate `public key`.\n\n```bash\nopenssl rsa -in snowflake-kafka-connector-private-rsa-key.p8 -pubout -out snowflake-kafka-connector-public-rsa-key.pub\n```\nSave the keys for later use.\n\n__7. Add the public key to the `kafka_connect_user`.__ \n\nIn the SQL worksheet from step 1:\n\n```sql\nALTER USER kafka_connector_user SET RSA_PUBLIC_KEY='MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIB.....';\n```\n\n__8. Setup Snowflake Kafka Connector__\n\n- Download `Snowflake Kafka Connector` jar file from [Maven Repository](https://mvnrepository.com/artifact/com.snowflake/snowflake-kafka-connector/3.1.0?_fsi=WP9Bbe6o&_fsi=WP9Bbe6o) and copy the jar file into kafka/libs/ folder.\n\n- Start `Kafka Connector` on local machine. In new terminal\n\n```bash\ncd kafka/bin\n./connect-distributed.sh ../config/connect-distributed.properties\n```\n\n- Create Kafka Connect `Sink Connector Configuration` file (`snowflake-kafka-connector-config.json`)\n\n```json\n{\n  \"name\": \"snowflake-kafka-yahoo-finance-connector\",\n  \"config\": {\n    \"connector.class\": \"com.snowflake.kafka.connector.SnowflakeSinkConnector\",\n    \"tasks.max\": \"1\",\n    \"topics\": \"yahoo-finance-topic\",\n\n    \"snowflake.url.name\": \"ACCOUNT-IDENTIFIER.snowflakecomputing.com:443\", -- replace ACCOUNT-IDENTIFIER\n    \"snowflake.user.name\": \"kafka_connector_user\",\n    \"snowflake.private.key\": \"MIIEuwIBADANBgkqhkiG9w0BAQEFAASCBKUwggShA.....\",\n    \"snowflake.database.name\": \"KAFKA_STREAMING\",\n    \"snowflake.schema.name\": \"YAHOO_FINANCE\",\n    \"snowflake.table.name\": \"stock_prices\",\n    \"snowflake.role.name\": \"kafka_role\",\n\n    \"key.converter\": \"org.apache.kafka.connect.storage.StringConverter\",\n    \"value.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\n    \"value.converter.schemas.enable\": \"false\",\n\n    \"buffer.count.records\": \"1000\",\n    \"buffer.flush.time\": \"10\",\n    \"buffer.size.bytes\": \"5000000\",\n\n    \"behavior.on.null.values\": \"IGNORE\"\n  }\n}\n```\n\n__9. Deploy configuration and create the Kafka Connector via REST API.__\n\nIn new terminal:\n\n```bash\ncurl -X POST -H \"Content-Type: application/json\" --data @snowflake-kafka-connector-config.json http://localhost:8083/connectors\n```"
  },
  {
   "cell_type": "markdown",
   "id": "4dd0fc6d-23b7-48bf-8270-7015a865058a",
   "metadata": {
    "name": "Ingest",
    "collapsed": false
   },
   "source": "## Publish live stock data from Yahoo Finance to yahoo-finance-topic Kafka topic\n"
  },
  {
   "cell_type": "markdown",
   "id": "8057eff5-4b64-4868-a43b-ca35b91923f9",
   "metadata": {
    "name": "Create_Kafka_Producer",
    "collapsed": false
   },
   "source": "__Create `kafka producer` to fetch live stock prices and publish them to Kafka.__\n\n__1. Install required Python packages.__\n\n```bash\npip install yfinance confluent_kafka\n```\n\n__2. Create Python script `kafka-producer.py` to fetch data and publish.__\n\n```python\nimport yfinance as yf \nfrom confluent_kafka import Producer \nimport json\nimport time\n\n# Kafka config\nproducer = Producer({'bootstrap.servers': 'localhost:9092'})\ntopic = 'yahoo-finance-topic'\n\n# List of stock symbols to track\nsymbols = ['SNOW', 'AMZN', 'GOOGL', 'MSFT']\n\ndef acked(err, msg):\n    if err is not None:\n        print(f\"Failed to deliver message: {err}\")\n    else:\n        print(f\"Published to {msg.topic()} [{msg.partition()}] @ offset {msg.offset()}\")\n\nwhile True:\n    for symbol in symbols:\n        stock = yf.Ticker(symbol)\n        data = stock.info  # full info\n        \n        message = {\n            'symbol': symbol,\n            'price': data.get('regularMarketPrice'),\n            'currency': data.get('currency'),\n            'time': time.strftime('%Y-%m-%d %H:%M:%S'),\n        }\n\n        producer.produce(topic, value=json.dumps(message), key=symbol, callback=acked)\n    \n    producer.flush()\n    time.sleep(30)  # Fetch every 30 seconds\n```\n\n__3. Run kafka-producer.py script.__\n\n```bash\npython kafka-producer.py\n```"
  },
  {
   "cell_type": "markdown",
   "id": "7baa60da-0a23-442c-ba7b-527c5450770c",
   "metadata": {
    "name": "Load",
    "collapsed": false
   },
   "source": "## Stream the data into a Snowflake table using the Kafka Connector\n"
  },
  {
   "cell_type": "markdown",
   "id": "70dbb92a-e801-4a77-a8a7-7cc5bf006314",
   "metadata": {
    "name": "Kafka_streaming_raw",
    "collapsed": false
   },
   "source": "Once the Kafka Producer is publishing Yahoo Finance data to the Kafka topic and the Snowflake Kafka Connector is running, Snowflake will automatically ingest data.\n\n__By default, the Snowflake Kafka Connector does not write directly to the specified target table `stock_prices` unless the Kafka message schema exactly matches the table schema.__\n\n__If the schema-matching mode is not enforced, Kafka auto-generates, by default__, in the KAFKA_STREAMING.YAHOO_FINANCE schema:\n\n- __A `staging table`__: \n```sql\nCREATE OR REPLACE TABLE KAFKA_STREAMING.YAHOO_FINANCE.YAHOO_FINANCE_TOPIC_1140052305 (\n    RECORD_METADATA VARIANT,\n    RECORD_CONTENT VARIANT)\n```\n\n- __An `internal stage` with client-side encryption and directory disabled__: \n```sql\nCREATE OR REPLACE STAGE SNOWFLAKE_KAFKA_CONNECTOR_SNOWFLAKE_KAFKA_YAHOO_FINANCE_CONNECTOR_430186219_STAGE_YAHOO_FINANCE_TOPIC_1140052305;\n```\n\n- __A `Pipe`__: \n\n```sql\nCREATE OR REPLACE PIPE KAFKA_STREAMING.YAHOO_FINANCE.SNOWFLAKE_KAFKA_CONNECTOR_SNOWFLAKE_KAFKA_YAHOO_FINANCE_CONNECTOR_430186219_PIPE_YAHOO_FINANCE_TOPIC_1140052305_0 auto_ingest=false as copy into yahoo_finance_topic_1140052305(RECORD_METADATA, RECORD_CONTENT) from (select $1:meta, $1:content from @SNOWFLAKE_KAFKA_CONNECTOR_snowflake_kafka_yahoo_finance_connector_430186219_STAGE_yahoo_finance_topic_1140052305 t) file_format = (type = 'json');\n```"
  },
  {
   "cell_type": "markdown",
   "id": "b4b2dbce-79c8-41da-bf03-005ca73f5ab8",
   "metadata": {
    "name": "Kafka_streaming_curated",
    "collapsed": false
   },
   "source": "__Automate the movement of data from the Kafka Connector auto-created staging table into the stock_prices target table using Snowflake Streams and Tasks.__ \n\nThis approach:\n\n- Avoids directly ingesting into the stock_prices curated table\n\n- Keeps staging and production concerns cleanly separated\n\n- Runs automatically at intervals (e.g., every 1 minute)\n\n__1. Identify the Auto-Generated Table__\n\n```sql\nSHOW TABLES LIKE '%_TOPIC_%';\n\nSELECT * FROM YAHOO_FINANCE_TOPIC_1140052305;\n\n-- TABLE name from SHOW TABLES\nSET kafka_staging_table = 'YAHOO_FINANCE_TOPIC_1140052305';\n```\n__2. Create a Stream on the Auto Table__\n\nThe stream tracks new rows inserted into the staging table by the Kafka Connector.\n\n```sql\nCREATE OR REPLACE STREAM kafka_finance_stream\nON TABLE IDENTIFIER($kafka_staging_table);\n```\n\n__3. Create a Task to Copy Data Every Minute__\n\n```sql\nCREATE OR REPLACE TASK move_kafka_data_to_snowflake_stock_prices\n  WAREHOUSE = COMPUTE_WH  \n  SCHEDULE = '1 MINUTE'\nAS\nINSERT INTO stock_prices (symbol, price, currency, time)\nSELECT\n  RECORD_CONTENT:\"symbol\"::STRING,\n  RECORD_CONTENT:\"price\"::FLOAT,\n  RECORD_CONTENT:\"currency\"::STRING,\n  RECORD_CONTENT:\"time\"::STRING\nFROM kafka_finance_stream;\n```\n\n__4. Start the task__\n\n```sql\nALTER TASK move_kafka_data_to_snowflake_stock_prices RESUME;\n```\nNow, Snowflake will:\n\n- Continuously ingest data into the auto-generated Kafka staging table\n\n- Use the stream to detect changes\n\n- Copy those rows into the curated stock_prices table every minute\n\n__5. Verification__ -- after 1 minute\n\n```sql\nSELECT * FROM stock_prices ORDER BY time DESC;\n```\n\n\n\n"
  },
  {
   "cell_type": "markdown",
   "id": "5c25729e-be96-4d48-8958-cb597f0c31ed",
   "metadata": {
    "name": "Transform",
    "collapsed": false
   },
   "source": "## Real-time or near-real-time analytics on stock prices"
  },
  {
   "cell_type": "markdown",
   "id": "1ed372b2-d080-4801-b4e8-a75ea970cc3b",
   "metadata": {
    "name": "Analytics",
    "collapsed": false
   },
   "source": "```sql\nUSE ROLE ACCOUNTADMIN;\nUSE DATABASE KAFKA_STREAMING;\nUSE SCHEMA YAHOO_FINANCE;\n```\n\n__1. Latest price per symbol__\n\n```sql\nCREATE OR REPLACE VIEW vw_latest_stock_prices AS\nSELECT symbol, price, time\nFROM (\n  SELECT *,\n         ROW_NUMBER() OVER (PARTITION BY symbol ORDER BY time DESC) AS rn\n  FROM stock_prices\n)\nWHERE rn = 1;\n```\n\n__2. Moving average (5-minute window)__\n\n```sql\nCREATE OR REPLACE DYNAMIC TABLE dt_moving_avg\nTARGET_LAG = '1 minute'\nWAREHOUSE = COMPUTE_WH\nAS\nWITH recent AS (\n  SELECT \n    symbol,\n    DATE_TRUNC('minute', time::TIMESTAMP_NTZ) AS minute_bucket,    \n    AVG(price) OVER (PARTITION BY symbol ORDER BY minute_bucket ROWS BETWEEN 4 PRECEDING AND CURRENT ROW) AS avg_price_5min\n  FROM stock_prices\n)\nSELECT *\nFROM recent;\n```\n\n__3. Anomaly detection - Price spike/dip detection (5% deviation from 5-row moving avg)__\n\n```sql\nCREATE OR REPLACE DYNAMIC TABLE dt_price_anomalies\nTARGET_LAG = '1 minute'\nWAREHOUSE = COMPUTE_WH\nAS\nWITH recent AS (\n  SELECT \n    symbol,\n    price,\n    time,\n    AVG(price) OVER (PARTITION BY symbol ORDER BY time ROWS BETWEEN 4 PRECEDING AND CURRENT ROW) AS moving_avg\n  FROM stock_prices\n)\nSELECT *\nFROM recent\nWHERE ABS(price - moving_avg) / NULLIF(moving_avg, 0) > 0.05;\n```\n\n__4. Stock leaderboard by latest price - ranks latest prices to show top movers or high-value stocks__\n\n```sql\nCREATE OR REPLACE VIEW vw_price_leaderboard AS\nSELECT symbol, price, RANK() OVER (ORDER BY price DESC) AS price_rank\nFROM (\n  SELECT symbol, price\n  FROM stock_prices\n  QUALIFY ROW_NUMBER() OVER (PARTITION BY symbol ORDER BY time DESC) = 1\n);\n```\n\nThe Snowflake QUALIFY clause is a tool used to filter results of window functions in SQL queries. Window functions perform calculations across a set of table rows related to the current row, and QUALIFY acts as an additional filter after these calculations. QUALIFY can improve query performance by reducing the need for subqueries and intermediate result sets. This can lead to faster query execution times and more efficient use of system resources, particularly in large datasets where performance is a critical concern.\n\nCheck refresh history:\n\n```sql\nSHOW DYNAMIC TABLES;\n\nSELECT *\nFROM TABLE(\n  INFORMATION_SCHEMA.DYNAMIC_TABLE_REFRESH_HISTORY(\n    NAME => 'KAFKA_STREAMING.YAHOO_FINANCE.dt_moving_avg',\n    RESULT_LIMIT => 20\n  )\n)\nORDER BY DATA_TIMESTAMP DESC;\n\nSELECT *\nFROM TABLE(\n  INFORMATION_SCHEMA.DYNAMIC_TABLE_REFRESH_HISTORY(\n    NAME => 'KAFKA_STREAMING.YAHOO_FINANCE.dt_price_anomalies',\n    RESULT_LIMIT => 20\n  )\n)\nORDER BY DATA_TIMESTAMP DESC;\n```"
  },
  {
   "cell_type": "markdown",
   "id": "a0b902d2-1910-4a79-a114-427f445e1722",
   "metadata": {
    "name": "Visualize",
    "collapsed": false
   },
   "source": "## Deliver live metrics in a Streamlit dashboard\n"
  },
  {
   "cell_type": "markdown",
   "id": "cd4c1b6f-5c3e-448f-a3bb-4b4b72fb122f",
   "metadata": {
    "name": "Streamlit_app",
    "collapsed": false
   },
   "source": "```python\nimport streamlit as st\nimport pandas as pd\nfrom snowflake.snowpark import Session\nimport plotly.express as px\n\n# Initialize Snowpark session\nsession = Session.get_active_session()\n   \nif st.button(\"ðŸ”„ Refresh Now\"):\n    st.rerun()\n\n# Cached data loading functions (auto-refresh every 30s)\n@st.cache_data(ttl=30)\ndef get_latest():\n    return session.table(\"vw_latest_stock_prices\").to_pandas()\n\n@st.cache_data(ttl=30)\ndef get_moving_avg():\n    return session.table(\"dt_moving_avg\").to_pandas()\n\n@st.cache_data(ttl=30)\ndef get_anomalies():\n    return session.table(\"dt_price_anomalies\").to_pandas()\n\n@st.cache_data(ttl=30)\ndef get_leaderboard():\n    return session.table(\"vw_price_leaderboard\").to_pandas()\n\n# Load data\nlatest = get_latest()\nmoving_avg = get_moving_avg()\nanomalies = get_anomalies()\nleaderboard = get_leaderboard()\n\n# Streamlit UI components\nst.title(\"ðŸ“ˆ Yahoo Finance Dashboard\")\n\n# Latest prices\nst.subheader(\"ðŸ’² Latest Prices\")\nst.dataframe(latest)\n\n# Leaderboard\nst.subheader(\"ðŸ† Price Leaderboard\")\nst.dataframe(leaderboard)\n\n\n# Moving Average Chart\nst.subheader(\"ðŸ“‰ 5-Minutes Moving Averages\")\n\n# Date filter\nmin_date = pd.to_datetime(moving_avg[\"MINUTE_BUCKET\"]).min().to_pydatetime()\nmax_date = pd.to_datetime(moving_avg[\"MINUTE_BUCKET\"]).max().to_pydatetime()\n\n# Set date slider\ndate_range = st.slider(\n    \"Date Range\",\n    min_value=min_date,\n    max_value=max_date,\n    value=(min_date, max_date),\n    format=\"YYYY-MM-DD HH:mm\"\n)\n\n# Set price filter\nprice_range = st.slider(\"Price Range\", float(moving_avg[\"AVG_PRICE_5MIN\"].min()), \n                        float(moving_avg[\"AVG_PRICE_5MIN\"].max()), \n                        (0.0, float(moving_avg[\"AVG_PRICE_5MIN\"].max())))\n\n# Symbol multi-select\navailable_symbols = sorted(moving_avg['SYMBOL'].unique().tolist())\nselected_symbols = st.multiselect(\"Select Symbols\", options=available_symbols, \n                                  default=available_symbols[:2])\n\n# Filter the DataFrame\nstart_date, end_date = date_range\n\nfiltered_data = moving_avg[\n    (moving_avg[\"SYMBOL\"].isin(selected_symbols)) &\n    (moving_avg[\"MINUTE_BUCKET\"] >= start_date) &\n    (moving_avg[\"MINUTE_BUCKET\"] <= end_date) &\n    (moving_avg[\"AVG_PRICE_5MIN\"] >= price_range[0]) &\n    (moving_avg[\"AVG_PRICE_5MIN\"] <= price_range[1])\n]\n\nfig = px.line(\n    filtered_data.sort_values([\"SYMBOL\", \"MINUTE_BUCKET\"]),\n    x=\"MINUTE_BUCKET\",\n    y=\"AVG_PRICE_5MIN\",\n    color=\"SYMBOL\",\n    title=\"Multi-Symbol 5-Minutes Moving Averages\", \n    render_mode='svg'\n)\nfig.update_layout(xaxis_title=\"Time\", yaxis_title=\"5-Min Avg Price\", height=500)\nst.plotly_chart(fig, use_container_width=True)\n\n# Anomalies\nst.subheader(\"ðŸš¨ Anomalies\")\nst.dataframe(anomalies[anomalies['SYMBOL'].isin(selected_symbols)])\n\n# Dashboard SQL (Copyable)\nst.markdown(\"### ðŸ§¾ Dashboard SQL\")\n\nsql_script = \"\"\"\n-- Latest Price\nCREATE OR REPLACE VIEW vw_latest_stock_prices AS\nSELECT symbol, price, time\nFROM (\n  SELECT *,\n         ROW_NUMBER() OVER (PARTITION BY symbol ORDER BY time DESC) AS rn\n  FROM stock_prices\n)\nWHERE rn = 1;\n\n-- 5-Min Moving Average\nCREATE OR REPLACE DYNAMIC TABLE dt_moving_avg\nTARGET_LAG = '1 minute'\nWAREHOUSE = COMPUTE_WH\nAS\nWITH recent AS (\n  SELECT \n    symbol,\n    DATE_TRUNC('minute', time::TIMESTAMP_NTZ) AS minute_bucket,    \n    AVG(price) OVER (PARTITION BY symbol ORDER BY minute_bucket ROWS BETWEEN 4 PRECEDING AND CURRENT ROW) AS avg_price_5min\n  FROM stock_prices\n)\nSELECT *\nFROM recent;\n\n-- Price Anomalies\nCREATE OR REPLACE DYNAMIC TABLE dt_price_anomalies\nTARGET_LAG = '1 minute'\nWAREHOUSE = COMPUTE_WH\nAS\nWITH recent AS (\n  SELECT \n    symbol,\n    price,\n    time,\n    AVG(price) OVER (PARTITION BY symbol ORDER BY time ROWS BETWEEN 4 PRECEDING AND CURRENT ROW) AS moving_avg\n  FROM stock_prices\n)\nSELECT *\nFROM recent\nWHERE ABS(price - moving_avg) / NULLIF(moving_avg, 0) > 0.05;\n\n-- Leaderboard by Latest Price\nCREATE OR REPLACE VIEW vw_price_leaderboard AS\nSELECT symbol, price, RANK() OVER (ORDER BY price DESC) AS price_rank\nFROM (\n  SELECT symbol, price\n  FROM stock_prices\n  QUALIFY ROW_NUMBER() OVER (PARTITION BY symbol ORDER BY time DESC) = 1\n);\n\"\"\"\n\nwith st.expander(\"ðŸ“‹ SQL Script\"):\n    st.code(sql_script, language=\"sql\")\n```"
  }
 ]
}